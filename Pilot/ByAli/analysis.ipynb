{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbec28f",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ae0153b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"conversations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "with open(\"demographics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    demographics = json.load(f)\n",
    "\n",
    "with open(\"nfr_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nfr_responses = json.load(f)\n",
    "\n",
    "with open(\"annotated_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nfr_responses = json.load(f)\n",
    "\n",
    "#with open(\"annotated_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    annotated_responses = json.load(f)\n",
    "\n",
    "with open(\"surveys.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    surveys = json.load(f)\n",
    "\n",
    "GT = {}\n",
    "with open(\"../../GT/NFR.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for responses in json.load(f):\n",
    "        for r in responses:\n",
    "            GT[r['id']] = r\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c92644df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr_responses[\"406bdbef-755b-4032-a82f-86e358987177\"] = nfr_responses[\"406bdbef-755b-4032-a82f-86e358987177\"][:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055b0c9",
   "metadata": {},
   "source": [
    "review attention questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "33fdf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr_responses_filtered = {}\n",
    "for pid, responses in nfr_responses.items():\n",
    "    nfr_responses_filtered[pid] = [r for r in responses if not r.get(\"is_attention_question\", False)]\n",
    "\n",
    "attention_questions = []\n",
    "for pid, responses in nfr_responses.items():\n",
    "    for r in responses:\n",
    "        if r.get(\"is_attention_question\", False):\n",
    "            attention_questions.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "82e12282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong attention questions in NFR list 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wrong attention questions in NFR list\", len([r for r in attention_questions if r['nfr_acknowledged']== True]))\n",
    "#[r for r in attention_questions if r['nfr_acknowledged']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f5c5d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong attention questions in feedback form 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wrong attention questions in feedback form\", len([r for r in attention_questions if not (r['q1_agreement'] == 'Disagree' and r['q2_agreement'] == 'Partially disagree' and r['q3_agreement']== 'Partially agree')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e858b",
   "metadata": {},
   "source": [
    "## Analysis 1. (agreement)\n",
    "We calculate the participants’ level of agreement by calculating the mean of their agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1bcc218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean satisfaction level: 4.3\n",
      "mean reasoning agreement: 4.3\n",
      "mean code location agreement: 4.6\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "satisfaction_level_agreement = 0\n",
    "reasoning_agreement = 0\n",
    "code_location_agreement = 0\n",
    "agreement_value = {\"Agree\": 5, \"Partially agree\": 4, \"Partially disagree\": 2, \"Disagree\": 1}\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for r in responses:\n",
    "        n += 1\n",
    "        satisfaction_level_agreement += agreement_value[r[\"q1_agreement\"]]\n",
    "        reasoning_agreement += agreement_value[r[\"q2_agreement\"]]\n",
    "        code_location_agreement += agreement_value[r[\"q3_agreement\"]]\n",
    "\n",
    "print(\"mean satisfaction level:\", satisfaction_level_agreement / n)\n",
    "print(\"mean reasoning agreement:\", reasoning_agreement / n)\n",
    "print(\"mean code location agreement:\", code_location_agreement / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f3f21",
   "metadata": {},
   "source": [
    "## Analysis 2. (accuracy of LLMs' evaluation of satisfaction level, reasoning, and code location)\n",
    "We first extract the LLM's responses on satisfaction level, reasoning, and code location by manually reviewing the dialogues.\n",
    "- For the satisfaction level, we calculate the F1 score between the extracted LLM response and the ground truth.\n",
    "- For the reasoning, we measure accuracy as the mean of the similarity between the LLM’s response and the ground truth. We calculate this similarity using ROUGE or BERTScore.\n",
    "- For code location, we compute the F1 score by defining true positives as |G ∩ P|, False Positives as |P \\ G|, and False Negatives as |G \\ P|, where G is the ground truth set, and P is the extracted Set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf801cb",
   "metadata": {},
   "source": [
    "### satisfaction level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8ca00ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "satisfavtion_levels = [\"satisfied\", \"weakly satisfied\", \"weakly denied\", \"denied\", \"na\"]\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        y1 = response[\"satisfaction_level\"].lower()\n",
    "        y2 = GT[id][\"satisfaction_level\"].lower()\n",
    "        if y2 not in satisfavtion_levels:\n",
    "            raise Exception(f'error {id} GT')\n",
    "        if y1 not in satisfavtion_levels:\n",
    "            raise Exception(f'error {id} {pid}')\n",
    "        y_true.append(y2)\n",
    "        y_pred.append(y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d78c1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score: 0.700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"Macro F1 score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e45d3",
   "metadata": {},
   "source": [
    "### reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e55703d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1404.75it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1653.22it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# pip install bert-score\n",
    "# pip install transformers\n",
    "# https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964\n",
    "#from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Example texts\n",
    "reference = \"This is a reference text example.\"\n",
    "candidate = \"This is a candidate text example.\"\n",
    "# BERTScore calculation\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "\n",
    "def get_bert_score(reference, candidate):\n",
    "    P, R, F1 = scorer.score([candidate], [reference])\n",
    "    return P, R, F1\n",
    "\n",
    "P, R, F1 = get_bert_score(reference, candidate)\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_bert_cosine_sim(text1, text2):\n",
    "    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "    return similarity\n",
    "\n",
    "text1 = \"This is an example text.\"\n",
    "text2 = \"This text contains an example sentence.\"\n",
    "similarity = get_bert_cosine_sim(text1, text2)\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90be04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.5424 ± 0.0577\n",
      "BERTScore P:  0.5234 ± 0.0440\n",
      "BERTScore R:  0.5688 ± 0.0917\n",
      "Cosine Sim:   0.7871 ± 0.0635\n"
     ]
    }
   ],
   "source": [
    "y_p = []\n",
    "y_r = []\n",
    "y_f = []\n",
    "y_s = []\n",
    "reasonings = []\n",
    "\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        y1 = response[\"reasoning\"]\n",
    "        y2 = GT[id][\"reasoning\"]\n",
    "        P, R, F1 = get_bert_score(y1, y2)\n",
    "        similarity = get_bert_cosine_sim(y1, y2)\n",
    "        y_p.append(P.item())\n",
    "        y_r.append(R.item())\n",
    "        y_f.append(F1.item())\n",
    "        y_s.append(similarity)\n",
    "        reasonings.append((y1, y2))\n",
    "\n",
    "\n",
    "print(f\"BERTScore F1: {np.mean(y_f):.4f} ± {np.std(y_f):.4f}\")\n",
    "print(f\"BERTScore P:  {np.mean(y_p):.4f} ± {np.std(y_p):.4f}\")\n",
    "print(f\"BERTScore R:  {np.mean(y_r):.4f} ± {np.std(y_r):.4f}\")\n",
    "print(f\"Cosine Sim:   {np.mean(y_s):.4f} ± {np.std(y_s):.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity:\n",
      "y1: form auth and role constraints guard /auth/* resources in webroot/web-inf/web.xml lines 35-314 and authdao.authenticatepassword uses salted hashes to admit only valid credentials (lines 503-513);\n",
      "y2: (1) users must log in with username and password (form login in web.xml). (2) all ephi lives under /auth/*; those urls require a valid login. so access is further restricted by role and url path in web.xml. (3) ephi is not only “by role” but also “this user can only see this patient’s data.” for example, in viewmyrecordsaction, the code uses the user’s id so they only get their own record.\n",
      "\n",
      "Lowest similarity:\n",
      "y1: emergencyrecordcontroller/emergencyrecordmysql load emergency data and log emergency_report_view.\n",
      "y2: emergency responders use a dedicated er role for viewing patient records during emergencies.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_idx = np.argmax(y_s)\n",
    "min_idx = np.argmin(y_s)\n",
    "\n",
    "\n",
    "print(\"Highest similarity:\")\n",
    "print(y_s[max_idx])\n",
    "print(\"y1:\", reasonings[max_idx][0])\n",
    "print(\"y2:\", reasonings[max_idx][1])\n",
    "print(\"\\nLowest similarity:\")\n",
    "print(y_s[min_idx])\n",
    "print(\"y1:\", reasonings[min_idx][0])\n",
    "print(\"y2:\", reasonings[min_idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ba6eb",
   "metadata": {},
   "source": [
    "### code locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900feed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both y1 and y2 are empty skipping\n",
      "both y1 and y2 are empty skipping\n",
      "BERTScore F1: 0.6181 ± 0.1346\n",
      "BERTScore P:  0.6336 ± 0.1420\n",
      "BERTScore R:  0.6144 ± 0.1537\n",
      "Cosine Sim:   0.7673 ± 0.1597\n"
     ]
    }
   ],
   "source": [
    "y_p = []\n",
    "y_r = []\n",
    "y_f = []\n",
    "y_s = []\n",
    "reasonings = []\n",
    "\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        # TODO\n",
    "        y1 = \";\".join(response[\"code_location\"])\n",
    "        y2 = \";\".join(GT[id][\"code_location\"])\n",
    "        if not y1 and not y2:\n",
    "            print('both y1 and y2 are empty skipping')\n",
    "            continue\n",
    "        if not y1:\n",
    "            print('y1 is empty skipping')\n",
    "            continue\n",
    "        if not y2:\n",
    "            print('y2 is empty skipping')\n",
    "            continue        \n",
    "        P, R, F1 = get_bert_score(y1, y2)\n",
    "        similarity = get_bert_cosine_sim(y1, y2)\n",
    "        y_p.append(P.item())\n",
    "        y_r.append(R.item())\n",
    "        y_f.append(F1.item())\n",
    "        y_s.append(similarity)\n",
    "        reasonings.append((y1, y2))\n",
    "\n",
    "print()\n",
    "print(f\"BERTScore F1: {np.mean(y_f):.4f} ± {np.std(y_f):.4f}\")\n",
    "print(f\"BERTScore P:  {np.mean(y_p):.4f} ± {np.std(y_p):.4f}\")\n",
    "print(f\"BERTScore R:  {np.mean(y_r):.4f} ± {np.std(y_r):.4f}\")\n",
    "print(f\"Cosine Sim:   {np.mean(y_s):.4f} ± {np.std(y_s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_idx = np.argmax(y_s)\n",
    "min_idx = np.argmin(y_s)\n",
    "\n",
    "\n",
    "print(\"Highest similarity:\")\n",
    "print(y_s[max_idx])\n",
    "print(\"y1:\", reasonings[max_idx][0])\n",
    "print(\"y2:\", reasonings[max_idx][1])\n",
    "print(\"\\nLowest similarity:\")\n",
    "print(y_s[min_idx])\n",
    "print(\"y1:\", reasonings[min_idx][0])\n",
    "print(\"y2:\", reasonings[min_idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afe61f",
   "metadata": {},
   "source": [
    "## Analysis 3.(significant dialogue costs)\n",
    "To determine which dialogue metrics (Table 1) significantly correlate with dialogue performance, we first manually review the dialogues to extract dialogue cost values. For example, the mean elapsed time is calculated by dividing the difference between the first and last message submission times in the chatbot by the number of tasks assigned to the user. Or, task completion is 1 when satisfaction level, reasoning, and code location are identified in the dialogues; otherwise, it's 0. The number of user initiatives is the number of times a user doesn't ask a follow-up question to the previous message, and so on. For task success, we calculate Cohen's kappa between the LLM's response and the ground truth. We then measure user satisfaction by summing the Likert scores in the post-study survey questions (Table 2). Based on the Paradise framework, the performance of a dialogue system (i.e., the prediction of user satisfaction) is modeled as a multiple linear regression of dialogue costs (independent variables). We can determine whether there is any significant relationship between the performance and any of the dialogue costs. A common procedure for testing the significance of variables in a multiple linear regression model is to run an 'Omnibus' test followed by 'Post-Hoc' tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33486f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, dialogues in conversations.items():\n",
    "    num_tasks = len(nfr_responses_filtered)\n",
    "    MET = (dialogues[-1][\"bot_time\"] - dialogues[0][\"user_time\"]) / num_tasks\n",
    "    MUM = len(dialogues) / num_tasks\n",
    "    Comp = 1 # TODO\n",
    "    MWT = 0\n",
    "    MRT = 0\n",
    "    NMR = 0 #\n",
    "    NUR = 0\n",
    "    Error = 0\n",
    "    for dialogue in dialogues:\n",
    "        MWT += dialogue[\"user_message\"]\n",
    "        MRT += dialogue[\"bot_reply\"]\n",
    "        if dialogue.get(\"NUR\"):\n",
    "            NUR += 1\n",
    "    MWT = MWT / len(dialogue)\n",
    "    MRT = MRT / len(dialogue)\n",
    "    URR = NUR / num_tasks\n",
    "\n",
    "    Task_Success = 1 # TODO\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
