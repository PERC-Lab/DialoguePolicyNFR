{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbec28f",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0153b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"annotated_data/conversations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "with open(\"data/demographics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    demographics = json.load(f)\n",
    "\n",
    "with open(\"annotated_data/nfr_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nfr_responses = json.load(f)\n",
    "\n",
    "with open(\"data/surveys.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    surveys = json.load(f)\n",
    "\n",
    "GT = {}\n",
    "with open(\"../../GT/NFR.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for responses in json.load(f):\n",
    "        for r in responses:\n",
    "            GT[r['id']] = r\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055b0c9",
   "metadata": {},
   "source": [
    "review attention questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fdf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr_responses_filtered = {}\n",
    "for pid, responses in nfr_responses.items():\n",
    "    nfr_responses_filtered[pid] = [r for r in responses if not r.get(\"is_attention_question\", False)]\n",
    "\n",
    "attention_questions = []\n",
    "for pid, responses in nfr_responses.items():\n",
    "    for r in responses:\n",
    "        if r.get(\"is_attention_question\", False):\n",
    "            attention_questions.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e12282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong attention questions in NFR list 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wrong attention questions in NFR list\", len([r for r in attention_questions if r['nfr_acknowledged']== True]))\n",
    "#[r for r in attention_questions if r['nfr_acknowledged']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c5d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong attention questions in feedback form 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wrong attention questions in feedback form\", len([r for r in attention_questions if not (r['q1_agreement'] == 'Disagree' and r['q2_agreement'] == 'Partially disagree' and r['q3_agreement']== 'Partially agree')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e858b",
   "metadata": {},
   "source": [
    "## Analysis 1. (agreement)\n",
    "We calculate the participants’ level of agreement by calculating the mean of their agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcc218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean satisfaction level: 4.666666666666667\n",
      "mean reasoning agreement: 4.7\n",
      "mean code location agreement: 4.866666666666666\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "satisfaction_level_agreement = 0\n",
    "reasoning_agreement = 0\n",
    "code_location_agreement = 0\n",
    "agreement_value = {\"Agree\": 5, \"Partially agree\": 4, \"Partially disagree\": 2, \"Disagree\": 1}\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for r in responses:\n",
    "        n += 1\n",
    "        satisfaction_level_agreement += agreement_value[r[\"q1_agreement\"]]\n",
    "        reasoning_agreement += agreement_value[r[\"q2_agreement\"]]\n",
    "        code_location_agreement += agreement_value[r[\"q3_agreement\"]]\n",
    "\n",
    "print(\"mean satisfaction level:\", satisfaction_level_agreement / n)\n",
    "print(\"mean reasoning agreement:\", reasoning_agreement / n)\n",
    "print(\"mean code location agreement:\", code_location_agreement / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f3f21",
   "metadata": {},
   "source": [
    "## Analysis 2. (accuracy of LLMs' evaluation of satisfaction level, reasoning, and code location)\n",
    "We first extract the LLM's responses on satisfaction level, reasoning, and code location by manually reviewing the dialogues.\n",
    "- For the satisfaction level, we calculate the F1 score between the extracted LLM response and the ground truth.\n",
    "- For the reasoning, we measure accuracy as the mean of the similarity between the LLM’s response and the ground truth. We calculate this similarity using ROUGE or BERTScore.\n",
    "- For code location, we compute the F1 score by defining true positives as |G ∩ P|, False Positives as |P \\ G|, and False Negatives as |G \\ P|, where G is the ground truth set, and P is the extracted Set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf801cb",
   "metadata": {},
   "source": [
    "### satisfaction level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca00ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "satisfavtion_levels = [\"satisfied\", \"weakly satisfied\", \"weakly denied\", \"denied\", \"na\"]\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        y1 = response[\"satisfaction_level\"].lower()\n",
    "        y2 = GT[id][\"satisfaction_level\"].lower()\n",
    "        if y2 not in satisfavtion_levels:\n",
    "            raise Exception(f'error {id} GT')\n",
    "        if y1 not in satisfavtion_levels:\n",
    "            raise Exception(f'error {id} {pid}')\n",
    "        y_true.append(y2)\n",
    "        y_pred.append(y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78c1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score: 0.460\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"Macro F1 score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e45d3",
   "metadata": {},
   "source": [
    "### reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55703d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1594.00it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1646.48it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# pip install bert-score\n",
    "# pip install transformers\n",
    "# https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964\n",
    "#from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Example texts\n",
    "reference = \"This is a reference text example.\"\n",
    "candidate = \"This is a candidate text example.\"\n",
    "# BERTScore calculation\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "\n",
    "def get_bert_score(reference, candidate):\n",
    "    P, R, F1 = scorer.score([candidate], [reference])\n",
    "    return P, R, F1\n",
    "\n",
    "P, R, F1 = get_bert_score(reference, candidate)\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_bert_cosine_sim(text1, text2):\n",
    "    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "    return similarity\n",
    "\n",
    "text1 = \"This is an example text.\"\n",
    "text2 = \"This text contains an example sentence.\"\n",
    "similarity = get_bert_cosine_sim(text1, text2)\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90be04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.5540 ± 0.0552\n",
      "BERTScore P:  0.5647 ± 0.0749\n",
      "BERTScore R:  0.5527 ± 0.0751\n",
      "Cosine Sim:   0.8056 ± 0.0617\n"
     ]
    }
   ],
   "source": [
    "y_p = []\n",
    "y_r = []\n",
    "y_f = []\n",
    "y_s = []\n",
    "reasonings = []\n",
    "\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        y1 = response[\"reasoning\"]\n",
    "        y2 = GT[id][\"reasoning\"]\n",
    "        P, R, F1 = get_bert_score(y1, y2)\n",
    "        similarity = get_bert_cosine_sim(y1, y2)\n",
    "        y_p.append(P.item())\n",
    "        y_r.append(R.item())\n",
    "        y_f.append(F1.item())\n",
    "        y_s.append(similarity)\n",
    "        reasonings.append((y1, y2))\n",
    "\n",
    "\n",
    "print(f\"BERTScore F1: {np.mean(y_f):.4f} ± {np.std(y_f):.4f}\")\n",
    "print(f\"BERTScore P:  {np.mean(y_p):.4f} ± {np.std(y_p):.4f}\")\n",
    "print(f\"BERTScore R:  {np.mean(y_r):.4f} ± {np.std(y_r):.4f}\")\n",
    "print(f\"Cosine Sim:   {np.mean(y_s):.4f} ± {np.std(y_s):.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cf5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity:\n",
      "[[0.88341343]]\n",
      "y1: FORM auth and role constraints guard /auth/* resources in WebRoot/WEB-INF/web.xml lines 35-314 and AuthDAO.authenticatePassword uses salted hashes to admit only valid credentials (lines 503-513);\n",
      "y2: (1) Users must log in with username and password (form login in web.xml). (2) All ePHI lives under /auth/*; those URLs require a valid login. So access is further restricted by role and URL path in web.xml. (3) ePHI is not only “by role” but also “this user can only see this patient’s data.” For example, in ViewMyRecordsAction, the code uses the user’s ID so they only get their own record.\n",
      "\n",
      "Lowest similarity:\n",
      "[[0.66126966]]\n",
      "y1: Audit logs track accesses but no cryptographic integrity checks or tamper-evident mechanisms for ePHI are provided.\n",
      "y2: There is no mechanism to corroborate that ePHI has not been altered or destroyed in an unauthorized manner.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_idx = np.argmax(y_s)\n",
    "min_idx = np.argmin(y_s)\n",
    "\n",
    "\n",
    "print(\"Highest similarity:\")\n",
    "print(y_s[max_idx])\n",
    "print(\"y1:\", reasonings[max_idx][0])\n",
    "print(\"y2:\", reasonings[max_idx][1])\n",
    "print(\"\\nLowest similarity:\")\n",
    "print(y_s[min_idx])\n",
    "print(\"y1:\", reasonings[min_idx][0])\n",
    "print(\"y2:\", reasonings[min_idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ba6eb",
   "metadata": {},
   "source": [
    "### code locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900feed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both y1 and y2 are empty skipping\n",
      "both y1 and y2 are empty skipping\n",
      "both y1 and y2 are empty skipping\n",
      "y2 is empty skipping\n",
      "y2 is empty skipping\n",
      "y1 is empty skipping\n",
      "y2 is empty skipping\n",
      "\n",
      "F1: 0.2942 ± 0.2315\n",
      "P:  0.2790 ± 0.2165\n",
      "R:  0.4022 ± 0.3929\n"
     ]
    }
   ],
   "source": [
    "y_p = []\n",
    "y_r = []\n",
    "y_f = []\n",
    "y_s = []\n",
    "reasonings = []\n",
    "\n",
    "for pid, responses in nfr_responses_filtered.items():\n",
    "    for response in responses:\n",
    "        id = response['nfr_id']\n",
    "        y1 = response[\"code_location\"]\n",
    "        y2 = GT[id][\"code_location\"]\n",
    "        \n",
    "        y1 = [i[:i.find(' ')] if ' ' in i else i for i in y1] #TODO\n",
    "        y2 = [i[:i.find(' ')] if ' ' in i else i for i in y2]\n",
    "        \n",
    "        if not y1 and not y2:\n",
    "            print('both y1 and y2 are empty skipping')\n",
    "            continue\n",
    "        if not y1:\n",
    "            print('y1 is empty skipping')\n",
    "            continue\n",
    "        if not y2:\n",
    "            print('y2 is empty skipping')\n",
    "            continue   \n",
    "        \n",
    "        y1 = set(y1)\n",
    "        y2 = set(y2)\n",
    "\n",
    "        # True Positive is intersection of y1 and y2\n",
    "        TP = len(y1.intersection(y2))\n",
    "        if TP == 0:\n",
    "            p = 0\n",
    "            r = 0\n",
    "            f1 = 0\n",
    "        else:\n",
    "            p = TP/len(y1)\n",
    "            r = TP/len(y2)\n",
    "            f1 = 2 * ((p*r)/(p+r))\n",
    "\n",
    "        y_p.append(p)\n",
    "        y_r.append(r)\n",
    "        y_f.append(f1)        \n",
    "        reasonings.append((y1, y2))\n",
    "\n",
    "        # Code Location, by similarity\n",
    "        #y1 = \";\".join(response[\"code_location\"])\n",
    "        #y2 = \";\".join(GT[id][\"code_location\"])\n",
    "        #if not y1 and not y2:\n",
    "        #    print('both y1 and y2 are empty skipping')\n",
    "        #    continue\n",
    "        #if not y1:\n",
    "        #    print('y1 is empty skipping')\n",
    "        #    continue\n",
    "        #if not y2:\n",
    "        #    print('y2 is empty skipping')\n",
    "        #    continue        \n",
    "        #P, R, F1 = get_bert_score(y1, y2)\n",
    "        #similarity = get_bert_cosine_sim(y1, y2)\n",
    "        #y_p.append(P.item())\n",
    "        #y_r.append(R.item())\n",
    "        #y_f.append(F1.item())\n",
    "        #y_s.append(similarity)\n",
    "        #reasonings.append((y1, y2))\n",
    "\n",
    "print()\n",
    "print(f\"F1: {np.mean(y_f):.4f} ± {np.std(y_f):.4f}\")\n",
    "print(f\"P:  {np.mean(y_p):.4f} ± {np.std(y_p):.4f}\")\n",
    "print(f\"R:  {np.mean(y_r):.4f} ± {np.std(y_r):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431e6f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity:\n",
      "0.6666666666666666\n",
      "y1: {'AccessDAO.java', 'SessionTimeoutListener.java'}\n",
      "y2: {'SessionTimeoutListener.java'}\n",
      "\n",
      "Lowest similarity:\n",
      "0\n",
      "y1: {'AuthDAO.java'}\n",
      "y2: {'createTables.sql'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_idx = np.argmax(y_f)\n",
    "min_idx = np.argmin(y_f)\n",
    "\n",
    "\n",
    "print(\"Highest similarity:\")\n",
    "print(y_f[max_idx])\n",
    "print(\"y1:\", reasonings[max_idx][0])\n",
    "print(\"y2:\", reasonings[max_idx][1])\n",
    "print(\"\\nLowest similarity:\")\n",
    "print(y_f[min_idx])\n",
    "print(\"y1:\", reasonings[min_idx][0])\n",
    "print(\"y2:\", reasonings[min_idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afe61f",
   "metadata": {},
   "source": [
    "## Analysis 3.(significant dialogue costs)\n",
    "To determine which dialogue metrics (Table 1) significantly correlate with dialogue performance, we first manually review the dialogues to extract dialogue cost values. For example, the mean elapsed time is calculated by dividing the difference between the first and last message submission times in the chatbot by the number of tasks assigned to the user. Or, task completion is 1 when satisfaction level, reasoning, and code location are identified in the dialogues; otherwise, it's 0. The number of user initiatives is the number of times a user doesn't ask a follow-up question to the previous message, and so on. For task success, we calculate Cohen's kappa between the LLM's response and the ground truth. We then measure user satisfaction by summing the Likert scores in the post-study survey questions (Table 2). Based on the Paradise framework, the performance of a dialogue system (i.e., the prediction of user satisfaction) is modeled as a multiple linear regression of dialogue costs (independent variables). We can determine whether there is any significant relationship between the performance and any of the dialogue costs. A common procedure for testing the significance of variables in a multiple linear regression model is to run an 'Omnibus' test followed by 'Post-Hoc' tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8049900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def ttoi(i):\n",
    "    return datetime.fromisoformat(i).timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33486f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "rows = []\n",
    "for pid, dialogues in conversations.items():\n",
    "    nfrs = nfr_responses_filtered[pid]\n",
    "    num_tasks = len(nfr_responses_filtered)\n",
    "    num_turn = len(dialogues)\n",
    "    ids = [i[\"nfr_id\"] for i in nfrs]\n",
    "    # TODO k\n",
    "    y1 = [i[\"satisfaction_level\"] for i in nfrs]\n",
    "    y2 = [GT[id][\"satisfaction_level\"] for id in ids]\n",
    "    K = cohen_kappa_score(y1, y2)\n",
    "    #MUM\n",
    "    MUM = len(dialogues) / num_tasks\n",
    "    #MET\n",
    "    y1 = ttoi(dialogues[-1][\"bot_time\"])\n",
    "    y2 = ttoi(dialogues[0][\"user_time\"])\n",
    "    MET = (y1 - y2) / num_tasks\n",
    "    # Comp\n",
    "    Comp = sum([i[\"comp\"] for i in nfrs])/ num_tasks\n",
    "    # NUI\n",
    "    NUI = sum(1 for i in dialogues if i[\"UI\"])\n",
    "    # MWT\n",
    "    MWT = sum(len(d[\"user_message\"]) for d in dialogues) / num_turn\n",
    "    # MRT\n",
    "    MRT = sum(ttoi(d[\"bot_time\"]) - ttoi(d[\"user_time\"]) for d in dialogues) / num_turn\n",
    "    # NRT NA\n",
    "    # NUR\n",
    "    NUR = sum(1 for i in dialogues if i[\"UR\"])\n",
    "    # NIR & IRR\n",
    "    NIR = sum(1 for i in dialogues if i[\"IR\"])\n",
    "    IRR = sum(1 for i in dialogues if i[\"IR\"]) / num_turn\n",
    "    # Error\n",
    "    Error = sum(1 for i in dialogues if i[\"Error\"])\n",
    "    # NHM & IRR\n",
    "    NHM = sum(1 for i in dialogues if i[\"HM\"])\n",
    "    HMR = sum(1 for i in dialogues if i[\"HM\"]) / num_turn  \n",
    "    # NCM & CMR TODO\n",
    "    #NCM = sum(1 for i in dialogues if i[\"CM\"])\n",
    "    #CMR = sum(1 for i in dialogues if i[\"CM\"]) / num_turn   \n",
    "    # NGD & GDR\n",
    "    NGD = sum(1 for i in dialogues if i[\"GD\"])\n",
    "    GDR = sum(1 for i in dialogues if i[\"GD\"]) / num_turn \n",
    "    # NRD & RDR\n",
    "    NRD = sum(1 for i in dialogues if i[\"RD\"])\n",
    "    RDR = sum(1 for i in dialogues if i[\"RD\"]) / num_turn \n",
    "    # NAR & ARR\n",
    "    NAR = sum(1 for i in dialogues if i[\"AR\"])\n",
    "    ARR = sum(1 for i in dialogues if i[\"AR\"]) / num_turn  \n",
    "    # \"Context Memory\": \"Y\",\n",
    "    Context_Memory = sum(1 for i in dialogues if i.get(\"Context Memory\", \"\"))\n",
    "    Self_correction = sum(1 for i in dialogues if i.get(\"Self-correction\", \"\"))\n",
    "    Self_affirmation = sum(1 for i in dialogues if i.get(\"Self-affirmation\", \"\"))\n",
    "    Proactive_Interaction = sum(1 for i in dialogues if i.get(\"Proactive Interaction\", \"\"))\n",
    "    Instruction_Clarification = sum(1 for i in dialogues if i.get(\"Instruction Clarification\", \"\"))\n",
    "\n",
    "    US = surveys[pid]\n",
    "    US_total = int(US[\"q1\"]) + int(US[\"q2\"]) + int(US[\"q3\"]) + int(US[\"q4\"]) + int(US[\"q5\"]) + int(US[\"q6\"]) + int(US[\"q7\"]) + int(US[\"q8\"]) \n",
    "    print(US_total)\n",
    "    rows.append({\n",
    "        \"pid\": pid, \"K\": K, \"MUM\": MUM, \"MET\": MET, \"Comp\": Comp,\n",
    "        \"NUI\": NUI, \"MWT\": MWT, \"MRT\": MRT, \"NUR\": NUR,\n",
    "        \"NIR\": NIR, \"IRR\": IRR, \"Error\": Error,\n",
    "        \"NHM\": NHM, \"HMR\": HMR, \"NGD\": NGD, \"GDR\": GDR,\n",
    "        \"NRD\": NRD, \"RDR\": RDR, \"NAR\": NAR, \"ARR\": ARR,\n",
    "        \"Context_Memory\": Context_Memory, \"Self_correction\": Self_correction,\n",
    "        \"Self_affirmation\": Self_affirmation, \"Proactive_Interaction\": Proactive_Interaction,\n",
    "        \"Instruction_Clarification\": Instruction_Clarification,\n",
    "        \"US\": US_total\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7c6cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OLS Regression Summary ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     US   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Fri, 13 Feb 2026   Prob (F-statistic):                nan\n",
      "Time:                        00:44:43   Log-Likelihood:                 91.634\n",
      "No. Observations:                   3   AIC:                            -177.3\n",
      "Df Residuals:                       0   BIC:                            -180.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                         0.0011        inf          0        nan         nan         nan\n",
      "K                            -0.0013        inf         -0        nan         nan         nan\n",
      "MUM                           0.0190        inf          0        nan         nan         nan\n",
      "MET                           0.0228        inf          0        nan         nan         nan\n",
      "Comp                         -0.0011        inf         -0        nan         nan         nan\n",
      "NUI                           0.0285        inf          0        nan         nan         nan\n",
      "MWT                           0.0284        inf          0        nan         nan         nan\n",
      "MRT                          -0.0585        inf         -0        nan         nan         nan\n",
      "NUR                                0        nan        nan        nan         nan         nan\n",
      "NIR                                0        nan        nan        nan         nan         nan\n",
      "IRR                                0        nan        nan        nan         nan         nan\n",
      "Error                              0        nan        nan        nan         nan         nan\n",
      "NHM                                0        nan        nan        nan         nan         nan\n",
      "HMR                                0        nan        nan        nan         nan         nan\n",
      "NGD                           0.0624        inf          0        nan         nan         nan\n",
      "GDR                           0.0012        inf          0        nan         nan         nan\n",
      "NRD                                0        nan        nan        nan         nan         nan\n",
      "RDR                                0        nan        nan        nan         nan         nan\n",
      "NAR                                0        nan        nan        nan         nan         nan\n",
      "ARR                                0        nan        nan        nan         nan         nan\n",
      "Context_Memory                0.0021        inf          0        nan         nan         nan\n",
      "Self_correction               0.0211        inf          0        nan         nan         nan\n",
      "Self_affirmation              0.0486        inf          0        nan         nan         nan\n",
      "Proactive_Interaction              0        nan        nan        nan         nan         nan\n",
      "Instruction_Clarification          0        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.000\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.290\n",
      "Skew:                          -0.135   Prob(JB):                        0.865\n",
      "Kurtosis:                       1.500   Cond. No.                         399.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The input rank is higher than the number of observations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "/opt/miniconda3/envs/dialogue/lib/python3.10/site-packages/statsmodels/base/model.py:1527: RuntimeWarning: invalid value encountered in multiply\n",
      "  cov_p = self.normalized_cov_params * scale\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "predictors = [c for c in df.columns if c not in ['pid', 'US']]\n",
    "# 2. Multiple regression\n",
    "X = df[predictors]\n",
    "X = sm.add_constant(X)\n",
    "y = df['US']\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(\"\\n=== OLS Regression Summary ===\")\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
